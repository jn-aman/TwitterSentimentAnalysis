{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TwitterSentimentAnalysis.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "4ybCqMXjhRXd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from keras.models import Sequential\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cg-FIbpDmRl2",
        "colab_type": "code",
        "outputId": "e54e8383-4766-4abe-dcd3-d9a8a632adc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "metadata": {
        "id": "rbcYYLNsiFWi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tweets = pd.read_csv(\"mydata.csv\",header = None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3dCnCfG3iNbR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "documents = np.array(tweets)[0:2000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8_VISvd-kssc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "documents = [(word_tokenize(word),sentiment) for word,sentiment in documents]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GfnPI_N8l96h",
        "colab_type": "code",
        "outputId": "2bee72f4-474f-4054-94bb-34a419707549",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "sample_text = \"Does This thing really work? Lets see.\"\n",
        "words = word_tokenize(sample_text.lower())\n",
        "words"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['does', 'this', 'thing', 'really', 'work', '?', 'lets', 'see', '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "metadata": {
        "id": "DCErYqQamg-t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z9C4VSH6mlJI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "def get_simple_pos(tag):\n",
        "    if(tag.startswith('J')):\n",
        "        return wordnet.ADJ\n",
        "    elif(tag.startswith('V')):\n",
        "        return wordnet.VERB\n",
        "    elif(tag.startswith('N')):\n",
        "        return wordnet.NOUN\n",
        "    elif(tag.startswith('R')):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hYd5eDCgmpR8",
        "colab_type": "code",
        "outputId": "72858cf8-220f-40d9-b4c0-0d5298322e0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "w = 'better'\n",
        "pos_tag([w])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('better', 'RBR')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "metadata": {
        "id": "qKj1Cmtlm29G",
        "colab_type": "code",
        "outputId": "ed1d0fb0-d2cb-4884-89b2-8ffa7d4bc6b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3621
        }
      },
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "stops = set(stopwords.words('english'))\n",
        "punctuations = list(string.punctuation)\n",
        "stops.update(punctuations)\n",
        "stops.update(['http'])\n",
        "stops"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'!',\n",
              " '\"',\n",
              " '#',\n",
              " '$',\n",
              " '%',\n",
              " '&',\n",
              " \"'\",\n",
              " '(',\n",
              " ')',\n",
              " '*',\n",
              " '+',\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '/',\n",
              " ':',\n",
              " ';',\n",
              " '<',\n",
              " '=',\n",
              " '>',\n",
              " '?',\n",
              " '@',\n",
              " '[',\n",
              " '\\\\',\n",
              " ']',\n",
              " '^',\n",
              " '_',\n",
              " '`',\n",
              " 'a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'http',\n",
              " 'i',\n",
              " 'if',\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " '{',\n",
              " '|',\n",
              " '}',\n",
              " '~'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "metadata": {
        "id": "aG8FdqlynELf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def clean_review(words):\n",
        "    output_words = []\n",
        "    for w in words:\n",
        "        if(w.lower() not in stops):\n",
        "            pos = pos_tag([w])\n",
        "            clean_word = lemmatizer.lemmatize(w,pos = get_simple_pos(pos[0][1]))\n",
        "            output_words.append(clean_word)\n",
        "    return output_words  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cUDF48VBnMAT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "documents = [(clean_review(document),category) for document,category in documents]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qyideezHnkgp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random \n",
        "random.shuffle(documents)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ef7oIFyrnsKW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "training_doc = documents[0:1500]\n",
        "testing_doc = documents[1500:2000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MK0JDpG4nxQE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_words = []\n",
        "for doc in training_doc:\n",
        "    all_words += doc[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5aISAaA5n3qZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "freq = nltk.FreqDist(all_words)\n",
        "common  = freq.most_common(500)\n",
        "features = [i[0] for i in  common]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ef1jrwjUoBPS",
        "colab_type": "code",
        "outputId": "248e0ad3-86c0-4047-bdad-4004da3168de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "features[0:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Modi', 'RT', 'PM', \"'s\", 'amp']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "metadata": {
        "id": "pXEp3xsCoGkM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_feature_dict(words):\n",
        "    word_set = set(words)\n",
        "    current_features = {}\n",
        "    for w in features:\n",
        "        current_features[w] = w in word_set    \n",
        "    return current_features   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-7iIRdyXoxwi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "training_data = [(get_feature_dict(doc),category) for doc,category in training_doc]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o5Nlod8ao1px",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def prepare_dataset(d,cat):\n",
        "  x_train = []\n",
        "  y_train = []\n",
        "  for k,v in d.items():\n",
        "    if v:\n",
        "      x_train.append(1)\n",
        "    else:\n",
        "      x_train.append(0)\n",
        "  y_train.append(bool(cat))\n",
        "  return x_train,y_train "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OWeDzS0ewGUF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train = []\n",
        "y_train = []\n",
        "for doc,category in training_data:\n",
        "  x,y = prepare_dataset(doc,category)\n",
        "  x_train.append(x)\n",
        "  y_train.append(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tY0Nz88SxECR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train = np.asarray(x_train)\n",
        "y_train = np.asarray(y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "14K9YMWBxQfx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def model():\n",
        "  model = Sequential()\n",
        "  model.add(Dense(64, activation='relu', input_dim=500))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.compile(optimizer='adam',\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rzMblOF40v_u",
        "colab_type": "code",
        "outputId": "3df274a0-3d42-41cb-9fc4-f04aaa79dc8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "y_train1 = to_categorical(y_train)\n",
        "y_train1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       ...,\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "metadata": {
        "id": "6LImS3jQyjtw",
        "colab_type": "code",
        "outputId": "0b0a16c8-6658-499b-8ac0-e5a2f7b29708",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "cell_type": "code",
      "source": [
        "model = model()\n",
        "model.fit(x_train,y_train,epochs = 20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1500/1500 [==============================] - 0s 268us/step - loss: 0.4694 - acc: 0.8567\n",
            "Epoch 2/20\n",
            "1500/1500 [==============================] - 0s 55us/step - loss: 0.2074 - acc: 0.9540\n",
            "Epoch 3/20\n",
            "1500/1500 [==============================] - 0s 55us/step - loss: 0.1196 - acc: 0.9713\n",
            "Epoch 4/20\n",
            "1500/1500 [==============================] - 0s 54us/step - loss: 0.0834 - acc: 0.9853\n",
            "Epoch 5/20\n",
            "1500/1500 [==============================] - 0s 57us/step - loss: 0.0629 - acc: 0.9900\n",
            "Epoch 6/20\n",
            "1500/1500 [==============================] - 0s 55us/step - loss: 0.0495 - acc: 0.9913\n",
            "Epoch 7/20\n",
            "1500/1500 [==============================] - 0s 59us/step - loss: 0.0401 - acc: 0.9953\n",
            "Epoch 8/20\n",
            "1500/1500 [==============================] - 0s 56us/step - loss: 0.0327 - acc: 0.9967\n",
            "Epoch 9/20\n",
            "1500/1500 [==============================] - 0s 56us/step - loss: 0.0274 - acc: 0.9973\n",
            "Epoch 10/20\n",
            "1500/1500 [==============================] - 0s 56us/step - loss: 0.0231 - acc: 0.9973\n",
            "Epoch 11/20\n",
            "1500/1500 [==============================] - 0s 58us/step - loss: 0.0200 - acc: 0.9980\n",
            "Epoch 12/20\n",
            "1500/1500 [==============================] - 0s 58us/step - loss: 0.0175 - acc: 0.9980\n",
            "Epoch 13/20\n",
            "1500/1500 [==============================] - 0s 58us/step - loss: 0.0156 - acc: 0.9980\n",
            "Epoch 14/20\n",
            "1500/1500 [==============================] - 0s 61us/step - loss: 0.0140 - acc: 0.9980\n",
            "Epoch 15/20\n",
            "1500/1500 [==============================] - 0s 59us/step - loss: 0.0128 - acc: 0.9980\n",
            "Epoch 16/20\n",
            "1500/1500 [==============================] - 0s 56us/step - loss: 0.0118 - acc: 0.9980\n",
            "Epoch 17/20\n",
            "1500/1500 [==============================] - 0s 58us/step - loss: 0.0109 - acc: 0.9980\n",
            "Epoch 18/20\n",
            "1500/1500 [==============================] - 0s 58us/step - loss: 0.0103 - acc: 0.9980\n",
            "Epoch 19/20\n",
            "1500/1500 [==============================] - 0s 58us/step - loss: 0.0097 - acc: 0.9980\n",
            "Epoch 20/20\n",
            "1500/1500 [==============================] - 0s 57us/step - loss: 0.0092 - acc: 0.9980\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fcc342aaba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "metadata": {
        "id": "W84KSaeqzpPt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "testing_data = [(get_feature_dict(doc),category) for doc,category in testing_doc]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "knSe0ca-4lLB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_test = []\n",
        "y_test = []\n",
        "for doc,category in testing_data:\n",
        "  x,y = prepare_dataset(doc,category)\n",
        "  x_test.append(x)\n",
        "  y_test.append(y)\n",
        "x_test = np.asarray(x_test)\n",
        "y_test = np.asarray(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "10g3gtw343by",
        "colab_type": "code",
        "outputId": "1b30b83d-20a3-4488-b048-0723af96674f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "score, acc = model.evaluate(x_test, y_test)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 0s 59us/step\n",
            "Test score: 0.004621944017708301\n",
            "Test accuracy: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}